{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fr'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Pendant quelques jours, le Nautilus s’écarta constamment de la côte américaine. Il ne voulait pas, évidemment, fréquenter les flots du golfe du Mexique ou de la mer des Antilles. Cependant, l’eau n’eût pas manqué sous sa quille, puisque la profondeur moyenne de ces mers est de dix-huit cents mètres ; mais, probablement ces parages, semés d’îles et sillonnés de steamers, ne convenaient pas au capitaine Nemo.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pendant quelques jours, le Nautilus s’écarta constamment de la côte américaine. Il ne voulait pas, évidemment, fréquenter les flots du golfe du Mexique ou de la mer des Antilles. Cependant, l’eau n’eût pas manqué sous sa quille, puisque la profondeur moyenne de ces mers est de dix-huit cents mètres ; mais, probablement ces parages, semés d’îles et sillonnés de steamers, ne convenaient pas au capitaine Nemo.'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Nautilus, golfe du Mexique, mer des Antilles, capitaine Nemo)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nautilus\n",
      "golfe du Mexique\n",
      "mer des Antilles\n",
      "capitaine Nemo\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print (entity.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nautilus LOC\n",
      "golfe du Mexique LOC\n",
      "mer des Antilles LOC\n",
      "capitaine Nemo PER\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print (entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Pendant\n",
      "Pendant\n",
      "ADP\n",
      "ADP___\n",
      "case\n",
      "Xxxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "quelques\n",
      "quelque\n",
      "DET\n",
      "DET__Definite=Ind|Number=Plur|PronType=Art\n",
      "det\n",
      "xxxx\n",
      "True\n",
      "True\n",
      "----------\n",
      "jours\n",
      "jour\n",
      "NOUN\n",
      "NOUN__Gender=Masc|Number=Plur\n",
      "obl\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      ",\n",
      ",\n",
      "PUNCT\n",
      "PUNCT___\n",
      "punct\n",
      ",\n",
      "False\n",
      "False\n",
      "----------\n",
      "le\n",
      "le\n",
      "DET\n",
      "DET__Definite=Def|Gender=Masc|Number=Sing|PronType=Art\n",
      "det\n",
      "xx\n",
      "True\n",
      "True\n",
      "----------\n",
      "Nautilus\n",
      "Nautilus\n",
      "PROPN\n",
      "PROPN__Gender=Masc|Number=Sing\n",
      "nsubj\n",
      "Xxxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "s’\n",
      "s’\n",
      "AUX\n",
      "AUX__Mood=Sub|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "amod\n",
      "x’\n",
      "False\n",
      "True\n",
      "----------\n",
      "écarta\n",
      "écarter\n",
      "ADJ\n",
      "ADJ__Gender=Masc|Number=Sing\n",
      "ROOT\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "constamment\n",
      "constamment\n",
      "ADV\n",
      "ADV___\n",
      "advmod\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "de\n",
      "de\n",
      "ADP\n",
      "ADP___\n",
      "case\n",
      "xx\n",
      "True\n",
      "True\n",
      "----------\n",
      "la\n",
      "le\n",
      "DET\n",
      "DET__Definite=Def|Gender=Fem|Number=Sing|PronType=Art\n",
      "det\n",
      "xx\n",
      "True\n",
      "True\n",
      "----------\n",
      "côte\n",
      "côte\n",
      "NOUN\n",
      "NOUN__Gender=Fem|Number=Sing\n",
      "obl\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "américaine\n",
      "américain\n",
      "ADJ\n",
      "ADJ__Gender=Fem|Number=Sing\n",
      "amod\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      ".\n",
      ".\n",
      "PUNCT\n",
      "PUNCT___\n",
      "punct\n",
      ".\n",
      "False\n",
      "False\n",
      "----------\n",
      "Il\n",
      "Il\n",
      "PRON\n",
      "PRON__Gender=Masc|Number=Sing|Person=3\n",
      "nsubj\n",
      "Xx\n",
      "True\n",
      "False\n",
      "----------\n",
      "ne\n",
      "ne\n",
      "ADV\n",
      "ADV__Polarity=Neg\n",
      "advmod\n",
      "xx\n",
      "True\n",
      "True\n",
      "----------\n",
      "voulait\n",
      "vouloir\n",
      "VERB\n",
      "VERB__Mood=Ind|Number=Sing|Person=3|Tense=Imp|VerbForm=Fin\n",
      "ROOT\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "pas\n",
      "pas\n",
      "ADV\n",
      "ADV__Polarity=Neg\n",
      "advmod\n",
      "xxx\n",
      "True\n",
      "True\n",
      "----------\n",
      ",\n",
      ",\n",
      "PUNCT\n",
      "PUNCT___\n",
      "punct\n",
      ",\n",
      "False\n",
      "False\n",
      "----------\n",
      "évidemment\n",
      "évidemment\n",
      "ADV\n",
      "ADV___\n",
      "advmod\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      ",\n",
      ",\n",
      "PUNCT\n",
      "PUNCT___\n",
      "punct\n",
      ",\n",
      "False\n",
      "False\n",
      "----------\n",
      "fréquenter\n",
      "fréquenter\n",
      "VERB\n",
      "VERB__VerbForm=Inf\n",
      "conj\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "les\n",
      "le\n",
      "DET\n",
      "DET__Definite=Def|Number=Plur|PronType=Art\n",
      "det\n",
      "xxx\n",
      "True\n",
      "True\n",
      "----------\n",
      "flots\n",
      "flot\n",
      "NOUN\n",
      "NOUN__Gender=Masc|Number=Plur\n",
      "obj\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "du\n",
      "du\n",
      "DET\n",
      "DET__Gender=Masc|Number=Sing\n",
      "det\n",
      "xx\n",
      "True\n",
      "True\n",
      "----------\n",
      "golfe\n",
      "golfe\n",
      "NOUN\n",
      "NOUN__Gender=Fem|Number=Plur\n",
      "obj\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "du\n",
      "du\n",
      "DET\n",
      "DET__Gender=Masc|Number=Sing\n",
      "det\n",
      "xx\n",
      "True\n",
      "True\n",
      "----------\n",
      "Mexique\n",
      "Mexique\n",
      "NOUN\n",
      "NOUN__Gender=Masc|Number=Sing\n",
      "obj\n",
      "Xxxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "ou\n",
      "ou\n",
      "CCONJ\n",
      "CCONJ___\n",
      "cc\n",
      "xx\n",
      "True\n",
      "True\n",
      "----------\n",
      "de\n",
      "de\n",
      "ADP\n",
      "ADP___\n",
      "case\n",
      "xx\n",
      "True\n",
      "True\n",
      "----------\n",
      "la\n",
      "le\n",
      "PRON\n",
      "PRON__Gender=Fem|Number=Sing|Person=3\n",
      "det\n",
      "xx\n",
      "True\n",
      "True\n",
      "----------\n",
      "mer\n",
      "mer\n",
      "VERB\n",
      "VERB__VerbForm=Inf\n",
      "conj\n",
      "xxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "des\n",
      "un\n",
      "DET\n",
      "DET__Definite=Ind|Number=Plur|PronType=Art\n",
      "det\n",
      "xxx\n",
      "True\n",
      "True\n",
      "----------\n",
      "Antilles\n",
      "Antilles\n",
      "NOUN\n",
      "NOUN__Number=Plur\n",
      "obj\n",
      "Xxxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      ".\n",
      ".\n",
      "PUNCT\n",
      "PUNCT___\n",
      "punct\n",
      ".\n",
      "False\n",
      "False\n",
      "----------\n",
      "Cependant\n",
      "Cependant\n",
      "ADV\n",
      "ADV___\n",
      "advmod\n",
      "Xxxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      ",\n",
      ",\n",
      "PUNCT\n",
      "PUNCT___\n",
      "punct\n",
      ",\n",
      "False\n",
      "False\n",
      "----------\n",
      "l’\n",
      "l’\n",
      "DET\n",
      "DET__Definite=Def|Gender=Fem|Number=Sing|PronType=Art\n",
      "det\n",
      "x’\n",
      "False\n",
      "True\n",
      "----------\n",
      "eau\n",
      "eau\n",
      "NOUN\n",
      "NOUN__Gender=Fem|Number=Sing\n",
      "nsubj\n",
      "xxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "n’\n",
      "n’\n",
      "ADJ\n",
      "ADJ__Gender=Fem|Number=Sing\n",
      "amod\n",
      "x’\n",
      "False\n",
      "False\n",
      "----------\n",
      "eût\n",
      "avoir\n",
      "AUX\n",
      "AUX__Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "cop\n",
      "xxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "pas\n",
      "pas\n",
      "ADV\n",
      "ADV__Polarity=Neg\n",
      "advmod\n",
      "xxx\n",
      "True\n",
      "True\n",
      "----------\n",
      "manqué\n",
      "manquer\n",
      "VERB\n",
      "VERB__Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part\n",
      "ROOT\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "sous\n",
      "sou\n",
      "ADP\n",
      "ADP___\n",
      "case\n",
      "xxxx\n",
      "True\n",
      "True\n",
      "----------\n",
      "sa\n",
      "son\n",
      "DET\n",
      "DET__Gender=Fem|Number=Sing|Poss=Yes\n",
      "nmod:poss\n",
      "xx\n",
      "True\n",
      "True\n",
      "----------\n",
      "quille\n",
      "quille\n",
      "NOUN\n",
      "NOUN__Gender=Fem|Number=Sing\n",
      "obl\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      ",\n",
      ",\n",
      "PUNCT\n",
      "PUNCT___\n",
      "punct\n",
      ",\n",
      "False\n",
      "False\n",
      "----------\n",
      "puisque\n",
      "puisque\n",
      "SCONJ\n",
      "SCONJ___\n",
      "mark\n",
      "xxxx\n",
      "True\n",
      "True\n",
      "----------\n",
      "la\n",
      "le\n",
      "DET\n",
      "DET__Definite=Def|Gender=Fem|Number=Sing|PronType=Art\n",
      "det\n",
      "xx\n",
      "True\n",
      "True\n",
      "----------\n",
      "profondeur\n",
      "profondeur\n",
      "NOUN\n",
      "NOUN__Gender=Fem|Number=Sing\n",
      "nsubj\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "moyenne\n",
      "moyenner\n",
      "ADJ\n",
      "ADJ__Gender=Fem|Number=Sing\n",
      "amod\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "de\n",
      "de\n",
      "ADP\n",
      "ADP___\n",
      "case\n",
      "xx\n",
      "True\n",
      "True\n",
      "----------\n",
      "ces\n",
      "ce\n",
      "DET\n",
      "DET__Number=Plur|PronType=Dem\n",
      "det\n",
      "xxx\n",
      "True\n",
      "True\n",
      "----------\n",
      "mers\n",
      "mer\n",
      "NOUN\n",
      "NOUN__Gender=Masc|Number=Plur\n",
      "nmod\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "est\n",
      "être\n",
      "AUX\n",
      "AUX__Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "cop\n",
      "xxx\n",
      "True\n",
      "True\n",
      "----------\n",
      "de\n",
      "de\n",
      "ADP\n",
      "ADP___\n",
      "case\n",
      "xx\n",
      "True\n",
      "True\n",
      "----------\n",
      "dix-huit\n",
      "dix-huit\n",
      "NOUN\n",
      "NOUN__Number=Sing\n",
      "nummod\n",
      "xxx-xxxx\n",
      "False\n",
      "True\n",
      "----------\n",
      "cents\n",
      "cent\n",
      "ADJ\n",
      "ADJ__Gender=Masc|Number=Plur\n",
      "ccomp\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "mètres\n",
      "métrer\n",
      "NOUN\n",
      "NOUN__Gender=Masc|Number=Plur\n",
      "amod\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      ";\n",
      ";\n",
      "PUNCT\n",
      "PUNCT___\n",
      "punct\n",
      ";\n",
      "False\n",
      "False\n",
      "----------\n",
      "mais\n",
      "mai\n",
      "CCONJ\n",
      "CCONJ___\n",
      "cc\n",
      "xxxx\n",
      "True\n",
      "True\n",
      "----------\n",
      ",\n",
      ",\n",
      "PUNCT\n",
      "PUNCT___\n",
      "punct\n",
      ",\n",
      "False\n",
      "False\n",
      "----------\n",
      "probablement\n",
      "probablement\n",
      "ADV\n",
      "ADV___\n",
      "advmod\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "ces\n",
      "ce\n",
      "DET\n",
      "DET__Number=Plur|PronType=Dem\n",
      "det\n",
      "xxx\n",
      "True\n",
      "True\n",
      "----------\n",
      "parages\n",
      "parage\n",
      "NOUN\n",
      "NOUN__Gender=Masc|Number=Plur\n",
      "obl\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      ",\n",
      ",\n",
      "PUNCT\n",
      "PUNCT___\n",
      "punct\n",
      ",\n",
      "False\n",
      "False\n",
      "----------\n",
      "semés\n",
      "semer\n",
      "VERB\n",
      "VERB__Gender=Masc|Number=Plur|Tense=Past|VerbForm=Part\n",
      "ROOT\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "d’\n",
      "d’\n",
      "PUNCT\n",
      "PUNCT___\n",
      "obj\n",
      "x’\n",
      "False\n",
      "True\n",
      "----------\n",
      "îles\n",
      "île\n",
      "ADJ\n",
      "ADJ__Gender=Fem|Number=Plur\n",
      "amod\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "et\n",
      "et\n",
      "CCONJ\n",
      "CCONJ___\n",
      "cc\n",
      "xx\n",
      "True\n",
      "True\n",
      "----------\n",
      "sillonnés\n",
      "sillonner\n",
      "VERB\n",
      "VERB__Gender=Masc|Number=Plur|Tense=Past|VerbForm=Part\n",
      "conj\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "de\n",
      "de\n",
      "ADP\n",
      "ADP___\n",
      "case\n",
      "xx\n",
      "True\n",
      "True\n",
      "----------\n",
      "steamers\n",
      "steamer\n",
      "NOUN\n",
      "NOUN__Gender=Masc|Number=Plur\n",
      "obl\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      ",\n",
      ",\n",
      "PUNCT\n",
      "PUNCT___\n",
      "punct\n",
      ",\n",
      "False\n",
      "False\n",
      "----------\n",
      "ne\n",
      "ne\n",
      "ADV\n",
      "ADV__Polarity=Neg\n",
      "advmod\n",
      "xx\n",
      "True\n",
      "True\n",
      "----------\n",
      "convenaient\n",
      "convenir\n",
      "VERB\n",
      "VERB__Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin\n",
      "advmod\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "pas\n",
      "pas\n",
      "ADV\n",
      "ADV__Polarity=Neg\n",
      "advmod\n",
      "xxx\n",
      "True\n",
      "True\n",
      "----------\n",
      "au\n",
      "au\n",
      "DET\n",
      "DET__Gender=Masc|Number=Sing|PronType=Dem\n",
      "obj\n",
      "xx\n",
      "True\n",
      "True\n",
      "----------\n",
      "capitaine\n",
      "capitaine\n",
      "NOUN\n",
      "NOUN__Gender=Fem|Number=Sing\n",
      "obj\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      "Nemo\n",
      "Nemo\n",
      "ADJ\n",
      "ADJ__Number=Sing\n",
      "flat:name\n",
      "Xxxx\n",
      "True\n",
      "False\n",
      "----------\n",
      ".\n",
      ".\n",
      "PUNCT\n",
      "PUNCT___\n",
      "punct\n",
      ".\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print (\"--\"*5)\n",
    "    print (token.text)\n",
    "    print (token.lemma_ )\n",
    "    print (token.pos_)\n",
    "    print (token.tag_)\n",
    "    print (token.dep_)\n",
    "    print (token.shape_)\n",
    "    print (token.is_alpha)\n",
    "    print (token.is_stop)\n",
    "    \n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2082205 ,  0.35524076, -0.5214021 , -0.84519553,  0.9475281 ,\n",
       "       -0.5026888 , -0.5306615 ,  0.85301745,  0.5566953 , -0.86541504,\n",
       "        0.4498696 ,  0.11506483, -0.19502872, -1.4779348 ,  0.94661355,\n",
       "       -2.0738144 ,  1.0767372 ,  1.279283  , -0.8239942 ,  1.5584826 ,\n",
       "       -0.32060874,  0.6150288 ,  0.35270202, -0.16390406,  1.3312045 ,\n",
       "       -0.4340963 ,  1.4210826 ,  0.35178596, -0.59423727,  0.41203448,\n",
       "       -0.51127565,  0.5666719 ,  1.0440773 , -1.2277607 ,  0.32438165,\n",
       "       -0.29850125, -0.33346137,  0.84699637, -1.9058113 ,  0.9671205 ,\n",
       "        1.0388346 ,  0.730374  , -0.41581133, -1.8293002 ,  1.458321  ,\n",
       "       -1.4891566 , -1.0749608 , -0.24257535, -1.0729876 ,  0.03666385,\n",
       "        0.04244671,  1.4374471 ,  0.09394549, -0.2132809 , -0.05612569,\n",
       "        0.98165506,  1.3288777 , -0.89869386,  1.4035624 ,  1.1532824 ,\n",
       "        1.347929  , -1.7014806 , -0.8097537 ,  0.80550224, -1.1199813 ,\n",
       "       -0.2077973 , -0.10977738,  0.2590254 ,  0.0442434 , -0.14289993,\n",
       "       -1.1698039 ,  1.4989417 , -1.0004352 ,  0.38413316,  1.618767  ,\n",
       "        0.22763188, -0.75819135, -0.9979754 ,  0.22228254, -0.02020942,\n",
       "        1.5990906 ,  0.41429907, -1.0815302 , -0.2541406 , -0.07719588,\n",
       "        0.05563572, -0.45463538,  0.41796824,  0.9948706 ,  1.0080749 ,\n",
       "       -1.6720412 ,  0.3086905 ,  0.06114232,  0.40890822,  1.6996732 ,\n",
       "       -1.2632393 , -0.43237102,  0.29304248, -0.49830854,  0.18400878,\n",
       "        0.737816  , -1.135018  , -0.4786972 , -1.5979539 , -0.5247879 ,\n",
       "       -1.1139485 , -0.7215545 ,  1.0958558 ,  0.94690377, -1.0072589 ,\n",
       "       -0.61075604, -1.2204528 , -2.1434495 ,  0.65108436, -1.4431633 ,\n",
       "        1.71707   ,  0.71149474, -1.0083799 , -0.32893577,  0.56812686,\n",
       "       -1.0208316 ,  1.0779345 ,  1.4153048 , -0.37309712, -0.72129714,\n",
       "        1.5061594 , -0.66312337,  0.94443834,  0.14518516,  0.5129072 ,\n",
       "       -0.14048268, -0.7979124 , -0.37142467,  0.6773757 , -0.51844776,\n",
       "       -0.22843555,  0.7465476 ,  0.09260835, -0.48222905, -0.29137415,\n",
       "       -0.6150748 ,  0.509301  ,  0.09017683,  0.1846059 , -0.30459273,\n",
       "       -0.6303078 , -0.69645613, -0.32154426,  0.30394694,  0.83880323,\n",
       "        0.4178138 ,  0.09336995, -0.27239737,  0.84656495,  0.19342643,\n",
       "       -0.20221506, -0.08570975,  0.0627676 ,  0.14142151,  0.0900741 ,\n",
       "       -0.42626995,  0.75945055, -0.46736804,  0.24788581,  0.8250369 ,\n",
       "       -0.0839686 ,  0.08571385,  0.07373675,  0.14996894, -0.65935105,\n",
       "        0.02864596,  0.34566242,  0.04791519, -0.23901717,  0.41177303,\n",
       "        0.04820305, -0.7994076 , -0.26798636, -0.51545686,  0.12664619,\n",
       "       -0.1669539 ,  0.6937773 , -0.00284761, -0.28865618, -0.1548503 ,\n",
       "        0.44191492, -0.49369302, -0.03757744,  0.651157  ,  0.00656049,\n",
       "        0.2370843 ,  0.25866243,  0.04639015, -0.03978259, -0.34480283,\n",
       "        0.59239525,  0.29161492,  0.34246513,  0.33001152, -0.33809003,\n",
       "        0.25179195,  0.03259838,  0.3962968 ,  0.2767815 , -0.5667505 ,\n",
       "        0.3881859 , -0.32265222,  0.1281081 , -0.7844262 , -0.56088495,\n",
       "        0.4745092 ,  0.14628798,  0.01506902,  0.30537295, -0.1673008 ,\n",
       "        0.07916341,  0.57240885, -0.562429  , -0.2561351 ,  0.07118956,\n",
       "       -0.14747937, -0.02061227, -0.9906845 , -0.33409387,  0.6684011 ,\n",
       "       -0.01375373,  0.19329965,  0.0316434 ,  0.22522211, -0.46936604,\n",
       "        0.39585578, -0.49818885,  0.15585566, -0.3059282 ,  0.05113689,\n",
       "        0.41288635,  0.15321656, -0.4328885 , -0.62646085, -0.08946285,\n",
       "       -0.7610068 ,  0.12828028,  0.27042496, -0.49539793, -0.60458094,\n",
       "       -0.55375254, -0.2908017 ,  0.25176558,  0.45741037, -0.21970116,\n",
       "        0.32471806,  0.7916825 , -0.36942765,  0.48764613, -0.76260555,\n",
       "        0.09378762,  0.06497432, -0.3139597 , -0.49828187, -1.0019435 ,\n",
       "       -0.9900729 , -0.11580743,  0.02918334, -0.22399558, -0.48440674,\n",
       "       -0.18188226,  0.02229773, -0.26239106,  0.45105818, -1.0310239 ,\n",
       "       -0.52953035,  0.5565485 , -1.0581303 ,  1.9413697 ,  0.22364958,\n",
       "       -0.528289  , -1.1941476 , -1.788359  ,  1.0367769 ,  0.7250746 ,\n",
       "        0.79593533, -0.12655362,  0.5083716 ,  1.4634255 ,  0.4969976 ,\n",
       "       -0.06962174,  0.71420753,  1.8838422 ,  0.18068196,  0.43927285,\n",
       "       -0.19832961,  1.1111295 ,  0.04867484,  0.03987863,  0.13291794,\n",
       "        0.13747416, -1.1527297 , -0.30092412,  1.741222  ,  0.90449184,\n",
       "        0.14814071, -0.24149011,  2.0046592 ,  0.6575212 ,  0.3976283 ,\n",
       "        0.22667825, -0.6931107 ,  0.6000489 ,  0.03133743, -0.4629971 ,\n",
       "        1.1136525 ,  0.32186776, -0.35268632,  0.04869369, -0.7475711 ,\n",
       "        1.122506  , -0.9213541 , -0.02649877, -0.49173835, -0.6358768 ,\n",
       "        0.7445058 , -0.30678582,  0.5472427 , -0.0721687 ,  2.326392  ,\n",
       "        0.5610831 ,  0.34107742, -0.51231176,  0.02219378, -0.25725842,\n",
       "       -0.8817097 ,  0.03446023, -0.4165687 ,  1.7906805 ,  0.52458674,\n",
       "        0.7266197 , -1.9075696 , -0.47597748,  0.48266163,  0.33465832,\n",
       "       -0.8097197 ,  1.6482962 , -0.46720675, -1.145381  , -0.48768672,\n",
       "       -0.02657346,  1.595932  , -0.8685434 ,  0.36456838, -1.8266662 ,\n",
       "       -0.2998881 , -0.64266217, -1.1141413 , -0.49650663, -0.7799051 ,\n",
       "       -0.33938542,  0.75140995, -1.3185916 , -0.45166737, -0.35911134,\n",
       "        0.6849994 ,  0.33539158, -0.2611429 ,  0.6365941 , -0.8493258 ,\n",
       "       -0.28885755, -0.22766382,  0.10473631, -1.2029619 , -0.24930415,\n",
       "        0.4723152 ,  0.61828375,  0.3324928 ,  0.31676188,  0.74252284,\n",
       "       -0.96647596,  0.60531145, -0.9858519 , -0.33968195,  1.9844965 ,\n",
       "        0.04581712, -0.5050919 , -0.72571766, -1.1599869 ], dtype=float32)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_01= \"Pendant quelques jours, le Nautilus s’écarta constamment de la côte américaine.\"\n",
    "text_02= \"Pendant quelques jours, le bateau s’approcha de la côte armoricaine.\"\n",
    "text_03= \"Pour faire un clafoutis aux prunaux il faut des prunaux.\"\n",
    "text_04= \"je traverse la rue et je vous trouve au boulot.\"\n",
    "\n",
    "d1 = nlp (text_01)\n",
    "d2 = nlp (text_02)\n",
    "d3 = nlp (text_03)\n",
    "d4 = nlp (text_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8691480824147785"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.similarity(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8691480824147785"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2.similarity(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.261903436104846"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3.similarity(d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5292775840533348"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3.similarity(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp('chien chat banane')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print (token1.text, token2.text, token1.similarity(token2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = './'\n",
    "\n",
    "filename = 'abeilles.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH + filename, sep=',', error_bad_lines=False)\n",
    "\n",
    "print(\"le fichier a {} rangées (abeilles) et {} colonnes\".format(df.shape[0], df.shape[1]))\n",
    "\n",
    "df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"likes\" ] .value_counts(). describe ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"mentions\"].value_counts ().describe ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tags'].value_counts ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doublons = df.duplicated ()\n",
    "doublons\n",
    "\n",
    "print (\"Y a t il au moins un vrai dans doublons: {}\".format (any (doublons)))\n",
    "\n",
    "if any (doublons):\n",
    "    print (\"il y a des doublons\")\n",
    "else:\n",
    "    print (\"il n'y a aucun doublon\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes =['author_handle', 'likes', 'mentions', 'permalink', 'shares',\n",
    "       'source_favorites', 'source_followers', 'source_following', 'tags',\n",
    "       'main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_to_liste(tags):\n",
    "    tags = tags.replace (\"'\", '')\n",
    "    tags = tags.replace (\"[]\", '').replace (\"]\", '')\n",
    "    \n",
    "   \n",
    "    tags = tags.split (',')\n",
    "    tags = [mot.strip() for mot in tags ]\n",
    "    return tags\n",
    "\n",
    "\n",
    "\n",
    "tags = \"['abeilles', 'néonicotinoides', 'pétition']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = \"['abeilles', 'néonicotinoides', 'pétition']\"\n",
    "   \n",
    "tags = tags.replace (\"'\", '')\n",
    "tags = tags.replace (\"[]\", '').replace (\"]\", '')\n",
    "tags = tags.split (',')\n",
    "tags = [mot.strip() for mot in tags ]\n",
    "\n",
    "print(type(tags))\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_liste = []\n",
    "\n",
    "for i, d in df[0:100].iterrows():\n",
    "    tags_liste = tags_liste + tags_to_liste(d.tags)\n",
    "print(tags_liste[0:20])\n",
    "print(len(tags_liste))\n",
    "\n",
    "print(\"le mot {} apparait {} fois dans la liste des hashtags\".format ('abeilles' , tags_liste.count('abeilles')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_unique = sorted(list(set(tags_liste)))\n",
    "print (tags_unique)\n",
    "len (tags_unique)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_occurence = []\n",
    "for tag in tags_unique:\n",
    "    liste_occurence.append(\n",
    "        {\n",
    "            \"tags\": tag,\n",
    "            \"nombre\": tags_liste.count(tag)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "df_occurence = pd.DataFrame(liste_occurence)\n",
    "    \n",
    "df_occurence.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_tweet'] = 0\n",
    "\n",
    "for i,d in df.iterrows():\n",
    "    df.loc[i, 'len_tweet' ] = len(d.main)\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"main\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,d in df.sample(n=10).iterrows():\n",
    "    print(\"-- ---\"*5)\n",
    "    tweet = d.main\n",
    "    doc = nlp(tweet)\n",
    "    print(tweet)\n",
    "    for token in doc:\n",
    "        print (token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = df[\"main\"].value_counts()\n",
    "\n",
    "def popularite_du_tweet(main)\n",
    "\n",
    "popularite_du_tweet = likes + shares\n",
    "\n",
    "\n",
    "return (main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_handle</th>\n",
       "      <th>likes</th>\n",
       "      <th>mentions</th>\n",
       "      <th>permalink</th>\n",
       "      <th>shares</th>\n",
       "      <th>source_favorites</th>\n",
       "      <th>source_followers</th>\n",
       "      <th>source_following</th>\n",
       "      <th>tags</th>\n",
       "      <th>main</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BleuBlancCoeur</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/BleuBlancCoeur/status/8724...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5430.0</td>\n",
       "      <td>6420</td>\n",
       "      <td>3885.0</td>\n",
       "      <td>['abeilles']</td>\n",
       "      <td>« Des carences en oméga-3 pourraient expliquer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VirginieDevigne</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.twitter.com/VirginieDevigne/status...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1302.0</td>\n",
       "      <td>154</td>\n",
       "      <td>723.0</td>\n",
       "      <td>['StopNéonics', 'abeilles']</td>\n",
       "      <td>.@EPhilippePM résistez à la Com° EU, signez le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17070908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.twitter.com/17070908/status/870697...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>30</td>\n",
       "      <td>127.0</td>\n",
       "      <td>['StopNéonics', 'abeilles']</td>\n",
       "      <td>.@N_Hulot résistez à la Com° EU, signez le déc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cocop64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.twitter.com/Cocop64/status/8710723...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['StopNeonics', 'abeilles']</td>\n",
       "      <td>.@AgnesBuzyn Arrêtez le massacre des #abeilles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mimi6werth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.twitter.com/mimi6werth/status/8710...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['StopNeonics', 'abeilles']</td>\n",
       "      <td>.@EPhilippePM Arrêtez le massacre des #abeille...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author_handle  likes mentions  \\\n",
       "0   BleuBlancCoeur    4.0      NaN   \n",
       "1  VirginieDevigne    0.0      NaN   \n",
       "2         17070908    0.0      NaN   \n",
       "3          Cocop64    0.0      NaN   \n",
       "4       mimi6werth    0.0      NaN   \n",
       "\n",
       "                                           permalink  shares  \\\n",
       "0  https://twitter.com/BleuBlancCoeur/status/8724...     5.0   \n",
       "1  https://www.twitter.com/VirginieDevigne/status...     0.0   \n",
       "2  https://www.twitter.com/17070908/status/870697...     0.0   \n",
       "3  https://www.twitter.com/Cocop64/status/8710723...     0.0   \n",
       "4  https://www.twitter.com/mimi6werth/status/8710...     0.0   \n",
       "\n",
       "   source_favorites  source_followers  source_following  \\\n",
       "0            5430.0              6420            3885.0   \n",
       "1            1302.0               154             723.0   \n",
       "2             188.0                30             127.0   \n",
       "3               0.0                 0               0.0   \n",
       "4               0.0                 0               0.0   \n",
       "\n",
       "                          tags  \\\n",
       "0                 ['abeilles']   \n",
       "1  ['StopNéonics', 'abeilles']   \n",
       "2  ['StopNéonics', 'abeilles']   \n",
       "3  ['StopNeonics', 'abeilles']   \n",
       "4  ['StopNeonics', 'abeilles']   \n",
       "\n",
       "                                                main  \n",
       "0  « Des carences en oméga-3 pourraient expliquer...  \n",
       "1  .@EPhilippePM résistez à la Com° EU, signez le...  \n",
       "2  .@N_Hulot résistez à la Com° EU, signez le déc...  \n",
       "3  .@AgnesBuzyn Arrêtez le massacre des #abeilles...  \n",
       "4  .@EPhilippePM Arrêtez le massacre des #abeille...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load la dataframe\n",
    "import pandas as pd\n",
    "df = pd.read_csv('abeilles.csv', sep=',', error_bad_lines=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main = df[\"main\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        « Des carences en oméga-3 pourraient expliquer...\n",
       "1        .@EPhilippePM résistez à la Com° EU, signez le...\n",
       "2        .@N_Hulot résistez à la Com° EU, signez le déc...\n",
       "3        .@AgnesBuzyn Arrêtez le massacre des #abeilles...\n",
       "4        .@EPhilippePM Arrêtez le massacre des #abeille...\n",
       "5        .@N_Hulot Arrêtez le massacre des #abeilles en...\n",
       "6        #abeilles en diminution nette dans mon jardin....\n",
       "7        Le #tilleul: une floraison essentielle pour la...\n",
       "8        #LFDay On y parle #agTech, #innovations #agri ...\n",
       "9        J'aime pas être interrompu dans mon taf, sauf ...\n",
       "10       .@AgnesBuzyn Arrêtez le massacre des #abeilles...\n",
       "11       #SaintJunien #SEDD2017 Animation-découverte su...\n",
       "12       .@MezardJacques résistez à la Com° EU, signez ...\n",
       "13       .@EPhilippePM résistez à la Com° EU, signez le...\n",
       "14       https://www.rts.ch/info/suisse/8672996-quand-l...\n",
       "15       Chez les #abeilles, qu'est ce qu'une #demoisel...\n",
       "16       La Commission européenne planche sur une inter...\n",
       "17       .@AgnesBuzyn résistez à la Com° EU, signez le ...\n",
       "18       .@MezardJacques Arrêtez le massacre des #abeil...\n",
       "19       4/5 «A Sainte-Clotilde, de fleur en buisson, a...\n",
       "20       les #abeilles agonisent mais le peuple s'en br...\n",
       "21       .@N_Hulot résistez à la Com° EU, signez le déc...\n",
       "22       Le Canada sur le point d'interdire un poison t...\n",
       "23       Aujourd'hui, je souhaitais partager cette très...\n",
       "24       Samedi 3 juin à #Lisses : découverte de la vie...\n",
       "25       .@N_Hulot Arrêtez le massacre des #abeilles en...\n",
       "26       @EMLYON on installe les #abeilles dans les ruc...\n",
       "27       Liste de plantes attractives pour les #abeille...\n",
       "28       #Coulogne :des riverains en conflit avec un ap...\n",
       "29       .@EPhilippePM Arrêtez le massacre des #abeille...\n",
       "                               ...                        \n",
       "11216    Vous avez manqué notre passage dans le #JT ? 😏...\n",
       "11217    Nous espérons effectivement que ça sera constr...\n",
       "11218    Des lycéens inventent une fleur \"connectée\" po...\n",
       "11219    #innovation #biodiversité 🐝Pour mieux étudier ...\n",
       "11220    Gestes irréversibles #abeilles #Bretagne Désem...\n",
       "11221    Des #abeilles sauvages à la maison http://www....\n",
       "11222    Des lycéens inventent une fleur \"connectée\" po...\n",
       "11223    🐝 #Initiative 🐝 Pour aider les #abeilles, 250....\n",
       "11224    Ce matin est prévu un nouveau contrôle de la s...\n",
       "11225    Oh oui.. on sent qu'elle est réceptive à #La43...\n",
       "11226    Bravo aux #abeilles et aux #apiculteurs @Orang...\n",
       "11227    A #Berlin des enfants se déguisent en #abeille...\n",
       "11228    Je trouve plus d'abeilles mortes chez moi qu'a...\n",
       "11229    #Dijon : c'était la fête des abeilles lors du ...\n",
       "11230    \"Les reines du Jardin Dominique Villars\" 🐝un a...\n",
       "11231    Les initiatives existent auprès des #abeilles ...\n",
       "11232    quelle blague ! #huiledepalme #abeilles #neoni...\n",
       "11233    Les vraies raisons de la bonne santé des #abei...\n",
       "11234    Une belle initaitive - cela se passe en Belgiq...\n",
       "11235    Des lycéens de Julliot de la Morandière à #Gra...\n",
       "11236    Le @gouvernement est donc responsable d'écocid...\n",
       "11237    @cat_mothron peut etre la raison de la mortali...\n",
       "11238    Les apiculteurs en colère pour défendre les #a...\n",
       "11239    RT @les_fees_nature: Si l'abeille disparaît......\n",
       "11240    Étude du comportement des #abeilles pour tente...\n",
       "11241    Pourquoi les #abeilles ne célèbrent pas la fêt...\n",
       "11242    Pourquoi les #abeilles ne célèbrent pas la fêt...\n",
       "11243    Des infos sur le site : http://beesavingpaper....\n",
       "11244    #ideesrecues #abeilles pour s ouvrir un peu l ...\n",
       "11245    \"Grâce aux #abeilles, 80% des espèces de plant...\n",
       "Name: main, Length: 11246, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(main)\n",
    "main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    « Des carences en oméga-3 pourraient expliquer...\n",
      "1    .@EPhilippePM résistez à la Com° EU, signez le...\n",
      "2    .@N_Hulot résistez à la Com° EU, signez le déc...\n",
      "3    .@AgnesBuzyn Arrêtez le massacre des #abeilles...\n",
      "4    .@EPhilippePM Arrêtez le massacre des #abeille...\n",
      "Name: main, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(main[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_01 = main.replace(\"#\", \" \")\n",
    "# main_02 = main_01.replace(\"\\r\", \" \")\n",
    "# main_03 = main_02.replace(\"  \", \" \")\n",
    "# main_04 = main_03.replace(\"#\", \"@\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2c8085ccae9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'main'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'main'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "for i,d in main.iteritems():\n",
    "    \n",
    "    d.loc[i, 'main'] = d['main'].replace(\"#\", \" \")\n",
    "    \n",
    "main.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in doc.ents:\n",
    "    print (entity.main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in doc.ents:\n",
    "    print (entity.main, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "main\n",
      "main\n",
      "NOUN\n",
      "NOUN__Gender=Fem|Number=Sing\n",
      "ROOT\n",
      "xxxx\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print (\"--\"*5)\n",
    "    print (token.text)\n",
    "    print (token.lemma_ )\n",
    "    print (token.pos_)\n",
    "    print (token.tag_)\n",
    "    print (token.dep_)\n",
    "    print (token.shape_)\n",
    "    print (token.is_alpha)\n",
    "    print (token.is_stop)\n",
    "    \n",
    "doc = nlp('main')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_01= \"Des spécialistes des -#chemtrails #contrails -détruisent l' #agriculture les #abeilles la qualité de nos #vies #criminels #bandits.\"\n",
    "main_02= \"Excellente nouvelle : notre amendement pour inclure dans la loi une définition des #néonicotinoïdes est adopté (légèrement modifié) ! #biodiversité #agriculture #abeilles #directAN.\"\n",
    "main_03= '''Immense satisfaction de l'@UNAFapiculture suite à cette nouvelle victoire de @genefutures \n",
    "    contre le #surfoxaflor, pesticide #néonicotinoïde toxique pour les #abeilles ! Une nouvelle qui devrait nous \n",
    "    permettre de passer un très bon week-end 😎🐝💪#StopNeonics #labeillevaincra Nouvelle victoire contre Dow Chemical \n",
    "    : le @Conseil_Etat confirme la suspension du Closer et du Transform, insecticides tueurs d'abeilles à base de\n",
    "    #sulfoxaflor #SaveTheDate #SaveTheBees.@N_Hulot: Sauvez les #abeilles, interdisez TOUS les pesticides \n",
    "    #néonicotinoïdes #sulfoxaflor #flupyradifurone'''\n",
    "main_04= \"Le gouvernement #macron en matière d' #ecologie c'est ca 👉#MakeThePlanetGreatAgain c'est juste un slogan. #abeilles.\"\n",
    "\n",
    "main_03\n",
    "\n",
    "b1 = nlp (main_01)\n",
    "b2 = nlp (main_02)\n",
    "b3 = nlp (main_03)\n",
    "b4 = nlp (main_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7396673259719577"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.similarity(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main main 1.0\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp('main')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print (token1.text, token2.text, token1.similarity(token2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les groupes de mots noun chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,chunk.root.head.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
